{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slope One Algorithm in PySpark\n",
    "\n",
    "Brendan Hawk  \n",
    "2022-06-21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Formal Definition of the problem: Predicting Recommendations\n",
    "\n",
    "Given a set of known ratings from Users A and B, and items I and J, can we predict what User B will rate Item J?\n",
    "\n",
    "| | Item 1 | Item 2 | Item 3 | ... | Item _i_ |\n",
    "|:-|-:|-:|-:|:-:|-:\n",
    "| User 1 | 1.5 | 4 | 2 | ... | 4 |\n",
    "| User 2 | 2 | **?** | 2 | ... | 3 |\n",
    "| User 3 | 5 | 3 | 3 | ... | **?** |\n",
    "| ... | ... | ... | ... | ... | ... |\n",
    "| User u | 1 | 2 | **?** | ... | 3.5 |\n",
    "\n",
    "\n",
    "The formal approach to this problem is often described in the context of applying direct mathematical solutions using _Matrix Decomposition_, where the full matrix (above) is decomposed or factorised into a set of two matrices of latent factors, one for the Users $H$ and one for the Items $W$ such that:\n",
    "\n",
    "$$ \\tilde{R} = HW $$\n",
    "\n",
    "These models, however they're generated, have one major drawback: even generating the full matrix of combinations above may be impossible or prohibitively slow due to the steep increase in size as the number of Users and Items grows. Even the smallest of the datasets included in this project would yield a matrix of 5,400,000 cells. The largest would be as large as 16,240,000,000 cells! This takes a considerable amount of operating memory and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slope One\n",
    "\n",
    "In a paper first published in 2005, two researchers[1] introduce the Slope One algorithm. In effect, the basic idea is to use prior knowledge of a user's previously rated items and other users' ratings for items in common with the first user and target item to generate a sort of matrix of deviations that can then be averaged into an approximation of the desired target prediction.\n",
    "\n",
    "For example:\n",
    "\n",
    "![a](https://upload.wikimedia.org/wikipedia/commons/c/cb/Simplicity_diagram.png)[2]\n",
    "\n",
    "To calculate $Pred_{(B, J)}$ we first find the deviation of user A's ratings from item I to item J, then apply it to the known rating for user B, item I. Since both User A and B have rated item I, we can use that deviation to predict our target rating:\n",
    "\n",
    "$$ Pred_{(B, J)} = R_{(B, I)} + (R_{(B, J)} - R_{(B, I)}) $$\n",
    "$$ Pred_{(B, J)} = 2 + (1.5 - 1) $$\n",
    "$$ Pred_{(B, J)} = 2.5 $$\n",
    "\n",
    "To go a step further, we can use the averages of this differences when there is more data to aggregate for our prediction. This approach uses a weighted average, giving more weight to the differences that are accumulated from a larger number of users who have rated both items:\n",
    "\n",
    "| | $i_1$ | $i_2$ | $i_3$ |\n",
    "|:-|-:|-:|-:|\n",
    "| $u_1$ | 3 | 5 | 4 |\n",
    "| $u_2$ | 3 | 2 | **?** |\n",
    "| $u_3$ | **?** | 2 | 4 |\n",
    "\n",
    "\n",
    "$$ Pred_{u_3,i_1} = \\frac{\\sum{((R_{u_3,i_n} + b) * support)}}{\\sum{support}} $$\n",
    "\n",
    "where $b$ is the average difference in ratings between items, and $support$ is the numerator for these averages (ie the number of times two items were both rated by a user):\n",
    "\n",
    "$$ b_{i_2, i_1} = \\frac{\\sum_n{R_{i_2} - R_{i_1}}}{n} $$\n",
    "\n",
    "In our example above we have two sets of $b$ and $support$:\n",
    "\n",
    "| Co-Rated Item | Target Item | $b$ | $support$ |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| $i_2$ | $i_1$ | ((5-3) + (2-3)) / 2 = 0.5 | 2 |\n",
    "| $i_3$ | $i_1$ | (4-3) / 1 = 1 | 1 |\n",
    "\n",
    "Finally, we can apply these averages and their weights to our known ratings for the target user:\n",
    "\n",
    "$$ Pred_{u_3,i_1} = \\frac{((R_{u_3,i_2} + b_{i_2,i_1}) * support_{i_2,i_1}) + ((R_{u_3,i_3} + b_{i_3,i_1}) * support_{i_3,i_1})}{support_{i_2,i_1}+support_{i_3,i_1}} $$\n",
    "$$ Pred_{u_3,i_1} = \\frac{((2+0.5)*2) + ((4+1)*1)}{2+1} $$\n",
    "$$ Pred_{u_3,i_1} = 3.33 $$\n",
    "\n",
    "\n",
    "[1]: Lemire, Daniel and Maclachlan, Anna (2007). Slope One Predictors for Online Rating-Based Collaborative Filtering. https://doi.org/10.48550/arXiv.cs/0702144  \n",
    "[2]: https://en.wikipedia.org/wiki/Slope_One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Slope One\n",
    "\n",
    "There are three main advantages to this algorithm that I find particularly compelling. First is the simplicity of its math. Matrix Decomposition approaches require based on much higher order math that can be difficult to understand and implement effectively. Current Big Data implementations also rely on a lot of iterative approximation, as in Gradient Descent, which requires time to train.\n",
    "\n",
    "Second is its understandability. You can summarise an explanation of this algorithm by describing that it takes all common comparisons where users have rated similar items and uses an average difference between them to approximate the target rating. This would be understandable and is explainable to anyone, even people well outside the realm of Data Science or other analytics fields.\n",
    "\n",
    "Lastly, and as someone with a software engineering background this is of particular interest to me: this model is easily updatable. the entire model is summarised in the values for $b$ and $support$, and if new data comes in (which it does, constantly!) you can update the model by simply accessing the model rows for the given item reconstitute new averages for the difference. This is only possible due to the storage of the $support$ value, so that you can always reverse the quotient of the average, add the new value to the numerator, increment the divisor, and calculate a new quotient. Other models mentioned here would be nearly impossible to update so succinctly, and would in effect require recalculating from scratch.\n",
    "\n",
    "### Caveats\n",
    "\n",
    "There are two main caveats to this approach. First, this approach still has relatively high memory requirements. While you no longer need to accomodate the entire matrix as described above, you need to have a matrix of all the differences as described. This is hard to calculate up front, as every user will have a different number of ratings and therefore combinations, but it should still be considerably less than a complete matrix as it is effectively like a dense variance matrix derived from a sparse matrix source.\n",
    "\n",
    "Second, it is possible that there is no data to build a prediction on. If a user has given no ratings, or if a user has given no ratings that can be tied to our model by combinations of other ratings with the target item, the prediction for this model will be blank. As a somewhat naive approach here I will calculate the mean score for each item to impute any missing predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Slope One to large datasets\n",
    "\n",
    "For this example, we will use the Latest Small MovieLens dataset from GroupLens[3]. This dataset contains 100,000 ratings on 9000 movies by 600 users\n",
    "\n",
    "There are two major steps to consider: building the model and formulating predictions. First, we will build our model.\n",
    "\n",
    "While Slope One effectively requires the storage of a full matrix of differences between all combinations of movies, it is easy to store this in a \"long\" format that is far easier to parallelize, rather than a true \"matrix\" object. The task is then a simple aggregation step to build the average of the differences and the count of ratings used to create that average.\n",
    "\n",
    "The first part of this is the expensive part. This is a partial cross join, where we take every user and find every combination of their ratings. In this implementation, order matters resulting in duplications of these pairs such as $(Item_1, Item_2)$ separate from $(Item_2, Item_1)$. There is room for improvement here in terms of memory or storage space used if you refactor the model to only store the \"lower half\" matrix of these differences (effectively, permutation instead of combination) and alter the final prediction algorithm accordingly as well.\n",
    "\n",
    "[3]: F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4: 19:1â€“19:19. https://doi.org/10.1145/2827872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, FloatType\n",
    "from pyspark.sql.functions import avg, col, sum, count\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get our Spark contexts ready\n",
    "conf = SparkConf().setAppName(\"Slope-One\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sparkSql = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw ratings data\n",
    "ratings = sparkSql.read.csv(\n",
    "    \"./data/100k.csv.bz2\",\n",
    "    header=False,\n",
    "    schema=StructType([\n",
    "        StructField(\"userId\", IntegerType(), False),\n",
    "        StructField(\"movieId\", IntegerType(), False),\n",
    "        StructField(\"rating\", FloatType(), False),\n",
    "        StructField(\"timestamp\", StringType(), False)\n",
    "    ])\n",
    ").select(\"userId\", \"movieId\", \"rating\")\n",
    "\n",
    "# Split the training and test sets\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2], seed=777)\n",
    "\n",
    "# We'll use this a couple times, so caching it will help the performance\n",
    "training.cache()\n",
    "\n",
    "training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the movie means.\n",
    "# There's a possibility the Slope One algorithm will not have enough data to make a prediction\n",
    "# for a given (user, movie), and the mean score for a movie becomes a simple naive fallback.\n",
    "movie_means = training.groupBy(\"movieId\").agg(avg(\"rating\").alias(\"mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, build the model.\n",
    "\n",
    "# First, create two references to the training data with columns renamed\n",
    "model_data_left = training.select(\n",
    "    \"userId\",\n",
    "    col(\"movieId\").alias(\"movieId1\"),\n",
    "    col(\"rating\").alias(\"rating1\")\n",
    ")\n",
    "model_data_right = training.select(\n",
    "    \"userId\",\n",
    "    col(\"movieId\").alias(\"movieId2\"),\n",
    "    col(\"rating\").alias(\"rating2\")\n",
    ")\n",
    "\n",
    "# This join will create rows for every combination of movies a user has rated,\n",
    "# for every user in the training dataset. In development, this has performed MUCH faster\n",
    "# than creating these combinations by hand using itertools.combinations or similar functions.\n",
    "model = (\n",
    "    model_data_left\n",
    "    .join(model_data_right, on = \"userId\", how = \"left\")\n",
    "    .filter(\"movieId1 != movieId2\")\n",
    ")\n",
    "\n",
    "model.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now we can aggregate the final model values for (b, support) for every combination created\n",
    "model = (\n",
    "    model\n",
    "    .withColumn(\"b\", col(\"rating2\") - col(\"rating1\"))\n",
    "    .groupBy(\"movieId1\", \"movieId2\")\n",
    "    .agg(sum(\"b\").alias(\"b\"), count(\"userId\").alias(\"support\"))\n",
    "    .withColumn(\"b\", col(\"b\") / col(\"support\"))\n",
    ")\n",
    "\n",
    "model.cache()\n",
    "model.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Predictions\n",
    "\n",
    "This step is actually the slow part. Slope One would be a much better choice for an application that doesn't try to compute entire datasets at once. It would have an easy implementation for a database-driven application, where you could query and calculate very small sets of data (ie for a single user and item) in parallel.\n",
    "\n",
    "However, I was able to implement this using PySpark to generate the entire set of missing ratings in our test dataset as follows. The aggregation of the data is once again a single easy step - the tricks are all in the joins beforehand, which can be very expensive in a distributed system.\n",
    "\n",
    "First, I've implemented the process for a single example. You could imaging replacing the filter steps on the `training` and `model` values with sql `SELECT ...` statements instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make understanding the prediction process easier, the following implementation creates one prediction.\n",
    "sample = (\n",
    "    ratings\n",
    "    .rdd\n",
    "    .takeSample(False, 1, seed = 2022)[0]\n",
    ")\n",
    "prediction_user = sample[\"userId\"]\n",
    "prediction_movie = sample[\"movieId\"]\n",
    "\n",
    "# First, get a reference to the model data for this movie\n",
    "prediction_model = model.filter(\"movieId2 = {}\".format(prediction_movie))\n",
    "\n",
    "(prediction_b, prediction_support) = (\n",
    "    training\n",
    "    # Get all ratings that this user has given for OTHER movies\n",
    "    .filter(\"userId = {} and movieId != {}\".format(prediction_user, prediction_movie))\n",
    "    # Join all model values for those movies\n",
    "    .withColumnRenamed(\"movieId\", \"movieId1\")\n",
    "    .join(prediction_model, on=\"movieId1\")\n",
    "    # Aggregate them into a single value for (b, support)\n",
    "    # For every row, calculate sum((user_rating + b) * support) and sum(support)\n",
    "    .agg(sum((col(\"rating\") + col(\"b\")) * col(\"support\")).alias(\"b\"), sum(col(\"support\")).alias(\"support\"))\n",
    "    .collect()[0]\n",
    ")\n",
    "\n",
    "final_prediction = prediction_b / prediction_support\n",
    "\n",
    "print(\"True Rating {:0.1f}\".format(sample[\"rating\"]))\n",
    "print(\"Predicted Rating {:0.1f}\".format(final_prediction))\n",
    "print(\"Loss {:0.4f}\".format(final_prediction - sample[\"rating\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this only takes a couple seconds, but even for the test sample of the small dataset (20,000 rows) this would mean a full runtime of nearly a day.\n",
    "\n",
    "Instead, we can replace the filter statements above with two more clever joins. These can be a bit complicated to reason and understand, but hopefully the comments on each join make the logic make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions for the entire test set relies on a series of somewhat complicated joins\n",
    "# First: Join the movie means (our fallback for missing predictions) and alias a few of the columns\n",
    "prediction_data = (\n",
    "    test\n",
    "    .join(movie_means, on=\"movieId\")\n",
    "    .select(\n",
    "        col(\"userId\").alias(\"user_pred\"),\n",
    "        col(\"movieId\").alias(\"movie_pred\"),\n",
    "        col(\"rating\").alias(\"rating_true\"),\n",
    "        \"mean\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Test data joined to movie means:\")\n",
    "prediction_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step joins the original training data (the co-ratings) for each user for all movies that are NOT the target movie\n",
    "training_join = [training.userId == prediction_data.user_pred, training.movieId != prediction_data.movie_pred]\n",
    "prediction_data = prediction_data.join(training, on=training_join, how=\"left\")\n",
    "\n",
    "print(\"Test data and means joined with original training data (co-ratings):\")\n",
    "prediction_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step joins the model data for the given (user, co-rating movie) combinations\n",
    "model_join = [model.movieId2 == prediction_data.movie_pred, model.movieId1 == prediction_data.movieId]\n",
    "prediction_data = prediction_data.join(model, on=model_join, how=\"left\")\n",
    "\n",
    "# Print out what this final table looks like before aggregation\n",
    "print(\"Test data, means, and co-ratings joined with model data - Complete dataFrame for generating predictions:\")\n",
    "print(prediction_data.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now aggregate all the predictions as above.\n",
    "predictions = (\n",
    "    prediction_data\n",
    "    .groupBy(\"user_pred\", \"movie_pred\", \"rating_true\", \"mean\")\n",
    "    # pred = sum((rating + b) * support) / sum(support)\n",
    "    .agg((sum((col(\"rating\") + col(\"b\")) * col(\"support\")) / sum(\"support\")).alias(\"rating_pred\"))\n",
    ")\n",
    "\n",
    "# Show some of our predictions\n",
    "print(\"Sample of Predictions:\")\n",
    "print(predictions.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any of our predictions are null, let's check them out\n",
    "print(\"Inspecting for missing predictions:\")\n",
    "print(predictions.filter(\"rating_pred IS NULL\").show(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss for the entire test set, here using RMSE as our loss function.\n",
    "loss = np.array(\n",
    "    predictions\n",
    "    .rdd\n",
    "    .map(lambda p: (p[\"rating_pred\"] - p[\"rating_true\"]) if p[\"rating_pred\"] else (p[\"mean\"] - p[\"rating_true\"]))\n",
    "    .collect()\n",
    ")\n",
    "loss = np.sqrt(np.sum(loss**2) / len(loss))\n",
    "print(\"RMSE: {:0.4f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "The Slope One algorithm has its advantages, but in truth increasing accuracy over other models is not one of them. In development, accuracy (RMSE) ranges from ~0.85 to ~0.88.\n",
    "\n",
    "For comparison, the `ALS` algorithm provided by PySpark ML has been implemented below. This uses all the defaults with no effort to tune the algorithm, and it performs just slightly worse than the Slope One algorithm above.\n",
    "\n",
    "Given this, and other examples I have worked with in the past, Slope One seems to perform equal with other single models and is wide open for improvements through the use of other data work (such as sweeping other means or effects out of the original data) or as part of an ensemble model (perhaps the aggregations could be weighted by commonalities between users, such as how many genres they both like, etc.). This, coupled with the other benefits above, make it a sound option for collaborative filtering and recommendation applications.\n",
    "\n",
    "I was able to run the included non-notebook version of this Slope One implementation on Google Compute using an `n1-highmem-2` for the master (2 vCPU, 13 GB memory) and 2 `n1-highmem-4` (4 vCPU, 26 GB memory) workers. Runtimes and Loss measures are listed below.\n",
    "\n",
    "| Dataset | Runtime | Loss |\n",
    "|:-|-:|-:|\n",
    "| 100k | ~1 min | 0.8782 |\n",
    "| 10M | ~15 min | 0.8562 |\n",
    "| 27M | ~60 min | 0.8624 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For comparison, a quick and basic usage of pysaprk.ml.recommendation.ALS \n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "# Build the recommendation model on the training data\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "model_als = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model_als.transform(test)\n",
    "\n",
    "# Calculate the final loss for comparison\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "loss_als = evaluator.evaluate(predictions)\n",
    "print(\"RMSE (ALS): {:0.4f}\".format(loss_als))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the cluster\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
